Absolument. On passe en mode 100% souverain, 100% local, 100% sur votre CPU. Aucun appel externe, aucune cl√© API payante (sauf pour vos propres services comme WordPress).

C'est un projet avanc√© mais extr√™mement gratifiant. Voici le guide complet, √©tape par √©tape.

### Philosophie et Attentes

*   **Souverainet√© Totale :** Aucune de vos donn√©es ou de vos requ√™tes ne quitte votre serveur. Vous contr√¥lez tout.
*   **Co√ªt Z√©ro (Logiciel) :** Tous les outils sont open source. Le seul co√ªt est celui de votre serveur et de l'√©lectricit√©.
*   **Compromis sur la Vitesse :** C'est le point crucial. **Attendez-vous √† une ex√©cution lente.** Chaque √©tape impliquant le LLM prendra du temps sur un CPU. La g√©n√©ration compl√®te d'un article peut facilement prendre 10 √† 20 minutes, voire plus. La patience est votre meilleure alli√©e.

---

### Architecture Globale 100% Locale

| Composant | R√¥le | Technologie Utilis√©e |
| :--- | :--- | :--- |
| **Serveur d'IA** | Fait tourner le mod√®le de langage | **Ollama** |
| **Moteur de Recherche** | Permet de chercher sur le web | **SearxNG** (m√©ta-moteur auto-h√©berg√©) |
| **Cerveau / Orchestrateur** | D√©finit et ex√©cute les agents | **CrewAI** (Script Python) |
| **Base de Donn√©es Contenu** | Stocke et publie les articles | **Votre propre site WordPress** |
| **Alerte / R√©vision** | Re√ßoit les articles rejet√©s | **Votre propre endpoint de Webhook** |

---

### √âtape 1 : Installation des Fondations (Ollama & SearxNG)

C'est la partie la plus technique. Nous allons utiliser Docker pour simplifier l'installation de SearxNG.

**1. Installez Docker et Docker Compose sur votre serveur**
Si ce n'est pas d√©j√† fait, suivez les instructions officielles pour votre distribution Linux. C'est un pr√©requis indispensable.

**2. Installez Ollama**
```bash
# T√©l√©charge et ex√©cute le script d'installation officiel
curl -fsSL https://ollama.com/install.sh | sh
```

**3. T√©l√©chargez et lancez le mod√®le LLM local**
Nous allons utiliser `llama3:8b`, un excellent compromis pour le CPU.
```bash
# Cette commande t√©l√©charge le mod√®le (plusieurs Go) et le rend disponible
ollama run llama3:8b
```
Une fois le mod√®le t√©l√©charg√©, vous pouvez arr√™ter le processus (`/bye`). Ollama continuera de tourner en service d'arri√®re-plan, pr√™t √† recevoir des requ√™tes.

**4. Installez et configurez le moteur de recherche local SearxNG**
Nous utilisons `docker-compose` car c'est la m√©thode la plus propre.
Cr√©ez un dossier pour votre projet, par exemple `~/autoblog`.
Dans ce dossier, cr√©ez un fichier nomm√© `docker-compose.yml` et collez-y le contenu suivant :

```yaml
# docker-compose.yml
version: '3.8'

services:
  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    restart: always
    ports:
      # Expose SearxNG sur le port 8080 de votre serveur
      - "8080:8080"
    volumes:
      # Stocke la configuration et les donn√©es de SearxNG de mani√®re persistante
      - ./searxng:/etc/searxng
    environment:
      # Vous pouvez d√©finir une instance publique si vous le souhaitez, mais pour un usage local, ce n'est pas n√©cessaire
      - SEARXNG_BASE_URL=http://localhost:8080/
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
```

Lancez SearxNG :
```bash
# Placez-vous dans le dossier contenant le fichier docker-compose.yml
cd ~/autoblog

# Lancez le service en arri√®re-plan
docker-compose up -d
```
Attendez une minute, puis v√©rifiez que tout fonctionne en visitant `http://VOTRE_IP_SERVEUR:8080` dans votre navigateur. Vous devriez voir l'interface de SearxNG.

---

### √âtape 2 : Pr√©paration de l'Environnement Python

```bash
# Cr√©ez et activez un environnement virtuel (bonne pratique)
python3 -m venv venv
source venv/bin/activate

# Installez toutes les librairies n√©cessaires
pip install crewai requests beautifulsoup4 langchain-community python-dotenv
```

---

### √âtape 3 : Le Code - Les Outils Personnalis√©s (`tools.py`)

C'est ici que nous cr√©ons notre propre outil de recherche qui va interroger notre instance locale de SearxNG.

Cr√©ez un fichier `tools.py` dans votre dossier `~/autoblog` :

```python
# tools.py
import requests
import json
from langchain.tools import BaseTool

# --- Outil de recherche 100% local ---
class SearxNGSearchTool(BaseTool):
    name: str = "Local Search Engine"
    description: str = "Indispensable pour faire des recherches sur internet. Utilise une instance locale de SearxNG pour trouver des informations r√©centes ou des articles de blog."

    def _run(self, query: str) -> str:
        """Ex√©cute une recherche sur l'instance locale de SearxNG."""
        try:
            searxng_url = "http://localhost:8080/"
            params = {
                'q': query,
                'format': 'json'
            }
            response = requests.get(searxng_url, params=params)
            response.raise_for_status()
            
            results = response.json().get('results', [])
            if not results:
                return "Aucun r√©sultat trouv√© pour cette recherche."

            # Formate les 3 premiers r√©sultats pour que le LLM puisse les utiliser
            summary = "Voici les r√©sultats de la recherche :\n"
            for i, res in enumerate(results[:3]):
                summary += f"R√©sultat {i+1}:\n"
                summary += f"  Titre: {res.get('title', 'N/A')}\n"
                summary += f"  URL: {res.get('url', 'N/A')}\n"
                summary += f"  Extrait: {res.get('content', 'N/A')}\n\n"
            return summary
        except requests.exceptions.RequestException as e:
            return f"Erreur de connexion au moteur de recherche local SearxNG : {e}"
        except Exception as e:
            return f"Une erreur est survenue lors de la recherche : {e}"

# --- Outils WordPress et Webhook (inchang√©s) ---

# Vos identifiants WordPress (√† stocker dans des variables d'environnement !)
WP_URL = "https://VOTRE_SITE.COM/wp-json/wp/v2"
WP_USER = "VOTRE_USER"
WP_PASSWORD = "VOTRE_MOT_DE_PASSE_APPLICATION" # Important: utilisez un mot de passe d'application

class WordPressTool(BaseTool):
    name = "WordPress Tool"
    description = "Indispensable pour interagir avec un site WordPress. Permet de r√©cup√©rer les tags existants et de publier de nouveaux articles."
    
    def _run(self, action: str, **kwargs):
        # ... (Le code de cette classe est identique √† la r√©ponse pr√©c√©dente) ...
        # ... (Copiez-collez le code de la classe WordPressTool ici) ...
        pass # Placeholder - remplacez par le vrai code

class WebhookTool(BaseTool):
    name = "Webhook Tool"
    description = "Utilis√© pour envoyer des donn√©es (article, note, raison) √† un endpoint sp√©cifique via une requ√™te POST."

    def _run(self, endpoint_url: str, data: dict):
        # ... (Le code de cette classe est identique √† la r√©ponse pr√©c√©dente) ...
        # ... (Copiez-collez le code de la classe WebhookTool ici) ...
        pass # Placeholder - remplacez par le vrai code

# Assurez-vous de copier le code complet pour WordPressTool et WebhookTool de la r√©ponse pr√©c√©dente.
```

---

### √âtape 4 : Le Code - Le Script Principal (`crew.py`)

Cr√©ez un fichier `crew.py` dans le m√™me dossier. C'est le cerveau de l'op√©ration.

```python
# crew.py
import os
import json
from crewai import Agent, Task, Crew, Process
from langchain_community.llms import Ollama
from crewai_tools import ScrapeWebsiteTool

# Importez VOS outils 100% locaux
from tools import SearxNGSearchTool, WordPressTool, WebhookTool

# --- Configuration du LLM Local ---
llm = Ollama(
    model="llama3:8b",
    base_url="http://localhost:11434"
)

# --- Initialisation des Outils ---
# On utilise notre outil maison au lieu de SerperDevTool
search_tool = SearxNGSearchTool() 
scrape_tool = ScrapeWebsiteTool()
wp_tool = WordPressTool()
webhook_tool = WebhookTool()

# --- D√©finition des Agents ---
# Les prompts sont les m√™mes, mais on pr√©cise les outils locaux

news_crawler = Agent(
    role="Veilleur d'Actualit√©s Tech utilisant des outils locaux",
    goal="Identifier un article pertinent et r√©cent en utilisant le moteur de recherche interne.",
    backstory="Expert en veille, tu te fies uniquement aux outils fournis pour explorer le web. Tu es m√©ticuleux et tu sais extraire l'URL la plus prometteuse des r√©sultats de recherche.",
    tools=[search_tool, scrape_tool], # Outils 100% locaux
    llm=llm,
    verbose=True
)

# Les autres agents (strategic_writer, creative_editor, qa_judge) sont identiques √† la r√©ponse pr√©c√©dente.
# Ils utilisent d√©j√† le `llm` local.
strategic_writer = Agent(...)
creative_editor = Agent(...)
qa_judge = Agent(...)

# Assurez-vous de copier le code de d√©finition de ces 3 agents ici.
# Le `qa_judge` doit avoir un prompt tr√®s strict pour le format JSON.

# --- D√©finition des T√¢ches ---
# Les t√¢ches sont identiques. On s'assure qu'elles utilisent les bons agents.
task_find_article = Task(...)
task_write_and_tag = Task(...)
task_enhance = Task(...)
task_judge = Task(...) # Le prompt de cette t√¢che est crucial pour la sortie JSON

# Assurez-vous de copier le code de d√©finition des 4 t√¢ches ici.

# --- Cr√©ation et Lancement du Crew ---
publishing_crew = Crew(
    agents=[news_crawler, strategic_writer, creative_editor, qa_judge],
    tasks=[task_find_article, task_write_and_tag, task_enhance, task_judge],
    process=Process.sequential,
    verbose=2
)

# --- Logique de D√©cision Finale (Chef d'Orchestre) ---
if __name__ == '__main__':
    print("üöÄ Lancement de l'√©quipe de r√©daction IA (Mode 100% LOCAL)...")
    print("üïí Soyez patient, le processus complet sur CPU est lent.")
    
    result_str = publishing_crew.kickoff()
    
    print("\n‚úÖ Crew a termin√© son travail. Analyse du r√©sultat...")
    
    # Nettoyage de la sortie du LLM local, qui peut √™tre "bavard"
    try:
        if "```json" in result_str:
            result_json_str = result_str.split("```json")[1].split("```")[0].strip()
        elif "```" in result_str:
             result_json_str = result_str.split("```")[1].split("```")[0].strip()
        else:
            result_json_str = result_str

        result = json.loads(result_json_str)
        
        # Le reste de la logique de d√©cision est identique √† la r√©ponse pr√©c√©dente...
        # Copiez-collez ici la partie `if score > 8:` etc.
        # ...
        
    except (json.JSONDecodeError, IndexError) as e:
        print(f"‚ùå Erreur: Le Juge n'a pas retourn√© un JSON valide ou le format est inattendu. Erreur: {e}")
        print("--- R√©sultat brut re√ßu du Crew ---")
        print(result_str)
        print("---------------------------------")
    except Exception as e:
        print(f"‚ùå Une erreur inattendue est survenue : {e}")

```
**N'oubliez pas de copier-coller les parties manquantes (`...`) depuis la r√©ponse pr√©c√©dente.**

---

### √âtape 5 : Lancement et Automatisation

1.  **Lancement manuel pour tester :**
    *   Assurez-vous que votre service `docker-compose` (SearxNG) et Ollama tournent.
    *   Activez votre environnement virtuel : `source venv/bin/activate`
    *   Lancez le script : `python crew.py`
    *   Observez la console et soyez patient.

2.  **Automatisation avec Cron :**
    *   Pour que votre bot travaille pour vous (par exemple, tous les jours √† 8h), utilisez un cron job.
    *   Ouvrez l'√©diteur de cron : `crontab -e`
    *   Ajoutez cette ligne (adaptez les chemins !) :
    ```bash
    # Ex√©cute le script de blog automatique tous les jours √† 8h00
    0 8 * * * /home/VOTRE_USER/autoblog/venv/bin/python /home/VOTRE_USER/autoblog/crew.py >> /home/VOTRE_USER/autoblog/cron.log 2>&1
    ```
    *   **Explication de la ligne :**
        *   `0 8 * * *` : S'ex√©cute √† 8h00, tous les jours.
        *   `/home/VOTRE_USER/autoblog/venv/bin/python` : Chemin **absolu** vers l'interpr√©teur Python de votre environnement virtuel. C'est crucial !
        *   `/home/VOTRE_USER/autoblog/crew.py` : Chemin **absolu** vers votre script.
        *   `>> ... cron.log 2>&1` : Redirige toute la sortie (normale et erreurs) vers un fichier de log pour que vous puissiez d√©boguer en cas de probl√®me.

Vous avez maintenant un syst√®me de publication de blog enti√®rement autonome, priv√© et gratuit, qui tourne sur votre propre mat√©riel. C'est le summum de l'automatisation "Do It Yourself" 
